{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d1e96d",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "- Using the other Jupyter notebooks, you've generated the following:\n",
    "    - .h5 files containing atomic point clouds\n",
    "    - CSV files containing targets and global descriptors (hMOFX-DB only)\n",
    "    \n",
    "___________\n",
    "\n",
    "\n",
    "- This notebook provides suggestions you want to follow to ensure compatibility with our training script and to prevent potential data leakages. We also provide the code for rotational data augmentation here. \n",
    "\n",
    "___________\n",
    "\n",
    "- **We recommend you to follow these steps for rotational data augmentation:** \n",
    "    - 1. Manual train/validation/test split, generating three CSV files with non-overlapping structure IDs\n",
    "    - 2. Perform rotational data upsampling for atomic point clouds (.h5). This will change the structure IDs.\n",
    "    - 3. Perform structure ID modifications (train first -> validation and test later using seperate scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a97a3",
   "metadata": {},
   "source": [
    "### Rotational Data Augmentation for Atomic Point Clouds (.h5)\n",
    "\n",
    "- This code introduces a modification to the structure IDs used to reference each atomic point cloud.\n",
    "- Depending on the degree of duplication, we add different numbers of atomic point clouds. \n",
    "- For instance, if we upsample by **adding** 5 rotational duplicates:\n",
    "    - This changes the ID of the canonical point cloud to {ID}_0.\n",
    "    - We further add rotated duplicates {ID}_1 ... {ID}_5 to the .h5 file.\n",
    "- Read the following section because this affects how training, validation, and test CSV files should be processed.\n",
    "\n",
    "### CAUTION: This code can generate a large file (~20-50 GB). Please run with enough storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea36704",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h5_path = \"\"    # Original atomic point cloud\n",
    "output_h5_path = \"\"   # (rotational upsampled)\n",
    "duplicates = 24       # set to 24 for all our experiments \n",
    "normalization = False # false for all our experiments (unit sphere normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266a9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def normalize_to_unit_sphere(point_cloud: np.ndarray) -> np.ndarray:\n",
    "    pc = point_cloud.copy()\n",
    "    xyz = pc[:, :3]\n",
    "    centroid = np.mean(xyz, axis=0)\n",
    "    xyz -= centroid\n",
    "    max_dist = np.max(np.linalg.norm(xyz, axis=1))\n",
    "    if max_dist > 0:\n",
    "        xyz /= max_dist\n",
    "    pc[:, :3] = xyz\n",
    "    return pc\n",
    "\n",
    "def random_rotation(point_cloud: np.ndarray, seed: int = None) -> np.ndarray:\n",
    "    pc = point_cloud.copy()\n",
    "    xyz = pc[:, :3]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rot = R.random(random_state=rng)\n",
    "    xyz_rot = rot.apply(xyz)\n",
    "    pc[:, :3] = xyz_rot\n",
    "    return pc\n",
    "\n",
    "def augment_h5_pointclouds(input_path: str, output_path: str, N: int, normalize: bool = True):\n",
    "    \n",
    "    with h5py.File(input_path, \"r\") as f_in, h5py.File(output_path, \"w\") as f_out:\n",
    "        global_seed = np.random.SeedSequence().entropy\n",
    "\n",
    "        for i, id_ in enumerate(f_in.keys()):\n",
    "            pc = f_in[id_][:]\n",
    "            id_str = id_.decode(\"utf-8\") if isinstance(id_, bytes) else str(id_)\n",
    "\n",
    "            # Normalize once if requested\n",
    "            base_pc = normalize_to_unit_sphere(pc) if normalize else pc\n",
    "\n",
    "            # Save original\n",
    "            f_out.create_dataset(f\"{id_str}_0\", data=base_pc)\n",
    "\n",
    "            # Generate N rotated versions\n",
    "            for j in range(1, N + 1):\n",
    "                seed = np.random.SeedSequence([global_seed, i, j]).generate_state(1)[0]\n",
    "                rotated = random_rotation(base_pc, seed=seed)\n",
    "                f_out.create_dataset(f\"{id_str}_{j}\", data=rotated)\n",
    "\n",
    "    print(f\"Augmented dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93767fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to /scratch/sk10275/AdsMOFNet-LIBRARY/0_final_submission_bench/datasets/jarvis_dft_2021_structures_up.h5\n"
     ]
    }
   ],
   "source": [
    "augment_h5_pointclouds(input_h5_path, output_h5_path, duplicates, normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1135ae",
   "metadata": {},
   "source": [
    "### Processing Target CSV Files \n",
    "\n",
    "- By performing rotatioanl data augmentation, we change the structural IDs of the point cloud as described above. \n",
    "- For the training set, we want to use all of these upsampled structure. Therefore, we apply the function **upsample_training_dataframe** to it, adding _0-N to the ID column.\n",
    "- For the validation and test sets, we want to only add _0 to the ID column because we want to use the original structures only. To this end, we apply **edit_val_test_dataframe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0a5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(\"\")\n",
    "val_df = pd.read_csv(\"\")\n",
    "test_df = pd.read_csv(\"\")\n",
    "\n",
    "structural_id_col = \"ID\" # set as 'ID' typically\n",
    "\n",
    "training_csv_upsampled_save_path = \"\"\n",
    "val_csv_edited_save_path = \"\"\n",
    "test_csv_edited_save_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d41a0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def upsample_training_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    N: int,\n",
    "    structural_id_col: str\n",
    ") -> pd.DataFrame:\n",
    "    if structural_id_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{structural_id_col}' not found in DataFrame.\")\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        base_id = row[structural_id_col]\n",
    "        for j in range(N + 1):\n",
    "            new_row = row.copy()\n",
    "            new_row[structural_id_col] = f\"{base_id}_{j}\"\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "\n",
    "def edit_val_test_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    structural_id_col: str\n",
    ") -> pd.DataFrame:\n",
    "    if structural_id_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{structural_id_col}' not found in DataFrame.\")\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df[structural_id_col] = new_df[structural_id_col].astype(str) + \"_0\"\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e1a88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_upsampled = upsample_training_dataframe(training_df, duplicates, structural_id_col)\n",
    "val_df_edited = edit_val_test_dataframe(val_df, structural_id_col)\n",
    "test_df_edited = edit_val_test_dataframe(test_df, structural_id_col)\n",
    "\n",
    "training_df_upsampled.to_csv(training_csv_upsampled_save_path, index=False)\n",
    "val_df_edited.to_csv(val_csv_edited_save_path, index=False)\n",
    "test_df_edited.to_csv(test_csv_edited_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5323a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mof-dl-pointnet2-v2",
   "language": "python",
   "name": "mof-dl-pointnet2-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
